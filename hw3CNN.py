import matplotlib.pyplot as plt
import numpy as np
import matplotlib 

def get_inputs(filename):
    arrays = np.load(filename)
    return arrays['x1'], arrays['x2'], arrays['t1'], arrays['t2']

#A1 passes the image through the kernel to get the first post-convolution layer
#A1 and A2 make up our new foward propogation functions for the CNN
#The output of A2 can be run through our old NN code
def A1(I, kernel_dim, kernel, bias):
    result_dim_0 = np.size(I,0)-kernel_dim+1
    result_dim_1 = np.size(I,1)-kernel_dim+1
    result = np.zeros((result_dim_0, result_dim_1))
    for i in range(result_dim_0):
        for j in range(result_dim_1):
            sum = 0
            for m in range(kernel_dim):
                for n in range(kernel_dim):
                    sum+=I[i+m,j+n]*kernel[m,n]
            result[i][j] = sum
    return result + bias

#A2 calculates the pooling layer from the post-convolution layer
#Namely, it just picks the highest value in each group and propogates it foward,
#remembering which ones it propogated
def A2(a1, pooling_dim):
    z1 = np.maximum(a1, 0) #apply relu if we haven't already
    result_dim_0 = int(np.size(z1,0)/pooling_dim)
    result_dim_1 = int(np.size(z1,1)/pooling_dim)
    result = np.zeros((result_dim_0, result_dim_1))
    z1_max_map = np.zeros((np.size(z1,0), np.size(z1,1)))
    for i in range(result_dim_0):
        for j in range(result_dim_1):
            temp = np.zeros((pooling_dim, pooling_dim))
            for p in range(pooling_dim):
                for q in range(pooling_dim):
                    temp[p,q] = z1[i*pooling_dim+p,j*pooling_dim+q]
            max_p, max_q = np.unravel_index(temp.argmax(), temp.shape)
            result[i,j] = temp[max_p,max_q]

            z1_max_map[i*pooling_dim+max_p,j*pooling_dim+max_q] = 1
    return result, z1_max_map

#delta k calculates the delta we propogate through the first post-convolutional layer generated by kernel k
def delta_k(z2_dim, delta_3, W3, z1_max_map, pooling_dim):
    delta_2 = np.zeros(z2_dim)
    for i in range(z2_dim):
        sum = 0
        for j in range(np.size(delta_3)):
            sum += delta_3.T[j]*W3[i, j]
        delta_2[i] = sum
    #print('delta_2', delta_2)  
    
    delta_1_dim0 = np.size(z1_max_map,0)
    delta_1_dim1 = np.size(z1_max_map,1)
    delta_1 = np.zeros((delta_1_dim0, delta_1_dim1))
    for i in range (delta_1_dim0):
        for j in range (delta_1_dim1):
            if(z1_max_map[i,j]==1):
                b_i = int(i/pooling_dim)
                b_j = int(j/pooling_dim)
                a2_dim0 = int(delta_1_dim0/pooling_dim)
                
                delta_1[i,j] = delta_2[b_i*a2_dim0+b_j]
    #print('delta_1', delta_1)
    return delta_1

#dkernel calculates the change in kernel given the delta and the image 
def dkernel(delta_1, I, kernel_dim):
    dkernel = np.zeros((kernel_dim, kernel_dim))
    delta_1_dim0 = np.size(delta_1,0)
    delta_1_dim1 = np.size(delta_1,1)
    
    for m in range (kernel_dim):
        for n in range (kernel_dim):
            sum = 0 
            for i in range (delta_1_dim0):
                for j in range (delta_1_dim1):
                    sum += delta_1[i][j]*I[i+m][j+m]
            dkernel[m][n] = sum
    return dkernel

def softmax(a):
     exp_scores = np.exp(a)
     return (exp_scores / np.sum(exp_scores, axis=1, keepdims=True)) 

'''
x is the vector of images
t is the classes of the images
H x W is the dimension of the images
K is the number of classes
kernel_dim is the dimension of the kernel
num_kernels is he number of kernels used
pooling_dim is the dimenson of the pooling layer
hidden_dim is the dimension of the hidden layer
learning_rate is the step size of the gradient descend
max_iter is the maximum number of iterations the program can run
'''
def build_model(x, t, H, W, K, kernel_dim, num_kernels, pooling_dim, hidden_dim, learning_rate, max_iter):
    #declare and initialize the 2 weight matrices and bias vectors
    np.random.seed(0)

    a1_H = H - kernel_dim + 1
    a1_W = W - kernel_dim + 1
    a2_H = int(a1_H/pooling_dim)
    a2_W = int(a1_W/pooling_dim)
    z2_D =  int(a2_H*a2_W)
    kernels = np.zeros((num_kernels, kernel_dim, kernel_dim))
    for i in range(num_kernels):
        kernels[i] = np.random.rand(kernel_dim, kernel_dim)/np.sqrt(kernel_dim)
    kernel_bias = np.zeros((num_kernels, a1_H, a1_W))

    W3 = np.random.randn(z2_D*num_kernels,hidden_dim)/np.sqrt(z2_D)
    b3 = np.zeros((1,hidden_dim))
    W4 = np.random.randn(hidden_dim,K)/np.sqrt(hidden_dim)
    b4 = np.zeros((1,K))
    total_error = 0.0

    for iter in range(max_iter):    
        
        total_error = 0.0
        prev_total_error = total_error
        #one pass of model training using stochastic gradient descend
        for i in np.random.permutation(len(train_x)):
            #forward propagation
            #1 convolutional layer with one kernel
            a1s = np.zeros((num_kernels, a1_H, a1_W))
            z1s = np.zeros((num_kernels, a1_H, a1_W))
            a2s = np.zeros((num_kernels, a2_H, a2_W))
            z2s = np.zeros((num_kernels, z2_D))
            z1_max_maps = np.zeros((num_kernels, a1_H, a1_W))
            
            for k in range(num_kernels):
                a1s[k]= A1(train_x[i], kernel_dim, kernels[k], kernel_bs[k])
                z1s[k]= np.maximum(a1s[k], 0)
                a2s[k], z1_max_maps[k] = A2(z1s[k], pooling_dim)
                z2s[k]= np.reshape(a2s[k], z2_D)

            z2combined = z2s[0] 
            for k in range(1, num_kernels):
                z2combined= np.concatenate((z2combined,  z2s[k]), axis=0)
            
            #fully connected layers
            #a3 = np.dot(z2, W3)+b3
            a3 = np.dot(z2combined, W3)+b3
            z3 = np.tanh(a3)
            a4 = np.dot(z3, W4)+b4
            y = softmax(a4) 
          
            #backward propagation
            delta_4 = y - train_t[i]
            delta_3 = (1 - np.power(z3, 2)) * (np.dot(delta_4, W4.T)) 
            delta_ks = np.zeros((num_kernels, a1_H, a1_W))
            for k in range(num_kernels):
                delta_ks[k] = delta_k(z2_D, delta_3, W3[range(k*z2_D, (k+1)*z2_D)], z1_max_maps[k], pooling_dim)
            
            dW4 = np.outer(z3, delta_4)
            dW3 = np.outer(z2combined, delta_3)
            W4 -= learning_rate*dW4
            W3 -= learning_rate*dW3  #update the the error fucnction as well lambda*w1^2numpy.random.randÂ¶ 
            db4 = delta_4
            db3 = delta_3
            b3 -= learning_rate*db3
            b4 -= learning_rate*db4
            
            dkernels = np.zeros((num_kernels, kernel_dim, kernel_dim))
            dkernel_bs = np.zeros((num_kernels, a1_H, a1_W))
            
            for k in range(num_kernels):
                dkernels[k] = dkernel(delta_ks[k], train_x[i], kernel_dim)
                kernels[k] -= learning_rate*dkernels[k] - reg_lambda*kernels[k]
                dkernel_bs[k] = delta_ks[k]
                kernel_bs[k] -= learning_rate*dkernel_bs[k]
        
            total_error += 0.5*np.dot(np.asarray(y - train_t[i]), np.asarray(y - train_t[i]).T)[0,0]
        
        print('iteration ', iter, 'total_error is ', total_error, 'prev_total_error', prev_total_error)
    return kernels, kernel_bs, W3, b3, W4, b4

def test_model(test_x, test_t, num_kernels, kernel_dim, pooling_dim, kernels, kernels_bs, W3, b3, W4, b4):
    num_errors = 0.0
    a1_H = H - kernel_dim + 1
    a1_W = W - kernel_dim + 1
    a2_H = int(a1_H/pooling_dim)
    a2_W = int(a1_W/pooling_dim)
    z2_D =  int(a2_H*a2_W)
    
    for i in np.random.permutation(len(test_x)): 
        a1s = np.zeros((num_kernels, a1_H, a1_W))
        z1s = np.zeros((num_kernels, a1_H, a1_W))
        a2s = np.zeros((num_kernels, a2_H, a2_W))
        z2s = np.zeros((num_kernels, z2_D))
        z1_max_maps = np.zeros((num_kernels, a1_H, a1_W))

        for k in range(num_kernels):
            a1s[k]= A1(test_x[i], kernel_dim, kernels[k], kernel_bs[k])
            z1s[k]= np.maximum(a1s[k], 0)
            a2s[k], z1_max_maps[k] = A2(z1s[k], pooling_dim)
            z2s[k]= np.reshape(a2s[k], z2_D)

        z2combined = z2s[0] 
        for k in range(1, num_kernels):
            z2combined= np.concatenate((z2combined,  z2s[k]), axis=0)

        a3 = np.dot(z2combined, W3)+b3
        z3 = np.tanh(a3)
        a4 = np.dot(z3, W4)+b4
        y = softmax(a4) 
        y = np.around(y) #round to [0, 1] or [1, 0]
        t = test_t[i]
        
        if (y[0,0] != t[0] or y[0,1] != t[1]):
            num_errors += 1
    print(num_errors)    
    error_rate = num_errors/len(test_t)
    return error_rate

if __name__ == "__main__":
    
    H = 6
    W = 6
    M = 24
    C = 2
    kernel_dim = 3
    num_kernels = 3
    pooling_dim  = 2
    learning_rate = 0.05
    max_iter = 10
    train_x1, train_x2, train_t1, train_t2 = get_inputs("hw3_train.npz")
    test_x1, test_x2, test_t1, test_t2 = get_inputs("hw3_test.npz")

    train_x = np.concatenate((train_x1, train_x2), axis=0)
    train_t = np.concatenate((train_t1, train_t2), axis=0)
    test_x = np.concatenate((test_x1, test_x2), axis=0)
    test_t = np.concatenate((test_t1, test_t2), axis=0)

    kernels, kernel_bs, W3, b3, W4, b4 = build_model(train_x, train_t, H, W, M, C, kernel_dim, num_kernels, pooling_dim, learning_rate, max_iter)
    train_errors = test_model(train_x, train_t, num_kernels, kernel_dim, pooling_dim, kernels, kernel_bs, W3, b3, W4, b4)
    test_errors = test_model(test_x, test_t, num_kernels, kernel_dim, pooling_dim, kernels, kernel_bs, W3, b3, W4, b4)
    print "train error rate: " + str(train_errors)
    print "test error rate: " + str(test_errors) 
